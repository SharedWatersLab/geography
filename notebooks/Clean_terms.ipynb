{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: load OSM/GIS-exported names data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1(a) merge the files I have - right now, Africa, Central America, South America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvs_to_merge = [\"OSMNames_export_tables/Africa.csv\", \"OSMNames_export_tables/CentralAmerica.csv\", \"OSMNames_export_tables/SouthAmerica.csv\"]\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for file in csvs_to_merge:\n",
    "    df = pd.read_csv(file)\n",
    "    merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "# can save it to a file too ... \n",
    "#merged_df.to_csv(\"merged_file.csv\", index=False)\n",
    "basins = merged_df[\"BCODE\"].unique() \n",
    "basins.shape # there are 132 basins from these three regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1(a) NEW ONES - from May 3 ish, Asia, Europe, North America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_eu_na = pd.read_csv(\"AS_EU_NA_frequency.csv\")\n",
    "basins_185 = as_eu_na['BCODE'].unique()\n",
    "# len(basins_185) # there are 185, added to 132 gets us 317 - so there are 4 (?) in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the other sheet in here\n",
    "af_sa_ca = pd.read_excel('AF_CA_SA_basin_terms.xlsx') # hard-coding the path is fine because I'm the only one running this, I think\n",
    "basins_129 = af_sa_ca['BCODE'].unique() \n",
    "#check if there's this term in the list\n",
    "#len(basins_129)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "was curious about the overlap between the two datasets.\n",
    "\n",
    "- should I merge them? yes :) \n",
    "- but! actually not until after some clean steps separately\n",
    "- this is because asia has a bunch of non-roman characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set1 = set(basins_185)\n",
    "# set2 = set(basins_129)\n",
    "# overlap_basins = set1.intersection(set2)\n",
    "# print(overlap_basins)\n",
    "# # curious if I should just merge in these overlap basins, remove them from eurasia/nam and add to af/cam\n",
    "# overlap_df = as_eu_na[as_eu_na['BCODE'].isin(list(overlap_basins))]\n",
    "#leftover_df = as_eu_na[~as_eu_na['BCODE'].isin(list(overlap_basins))]\n",
    "# so these can tolerate a clean step - removing water-related and non-roman\n",
    "#new_north = pd.DataFrame(leftover_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_north.to_csv(\"new_north_may7.csv\") # this is stored so that I can refer to it later without needing to run the above code\n",
    "# and then uncomment and run later when I'm ready to deal with \"new north\"\n",
    "\n",
    "#asia_europe_namerica = pd.read_csv(\"new_north_may7.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now what's left is new_south and new_north\n",
    "- \"south\" refers to South America, Central America, and Africa\n",
    "- \"north\" refers to North America, Europe, and Asia\n",
    "- there were six basins {'SUCT', 'HOND', 'CTAT', 'GJLV', 'NILE', 'CDLR'} in both groups\n",
    "- \"new south\" includes all overlap. those six basins are in new south.\n",
    "- \"new north\" removes all overlap. those six basins are not in new north.\n",
    "- \n",
    "- \n",
    "* -  this decision was made solely because (in early May) I have made progress on the \"south\" basins first (the files were smaller and easier to work with)\n",
    "* -  another factor in this decision is that these overlap basins/names appeared to have similar languages (in the water-related terms) and similar presence of non-roman characters as the \"south\" basins, so I felt that our current work flow would apply\n",
    "* -  new_north is likely to pose different challenges when it comes to non-roman characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>BCODE</th>\n",
       "      <th>name</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>AKPA</td>\n",
       "      <td></td>\n",
       "      <td>drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>AKPA</td>\n",
       "      <td></td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>AKPA</td>\n",
       "      <td></td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Afaẖ</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpakorum</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17532</th>\n",
       "      <td>4</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>ditch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17533</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17534</th>\n",
       "      <td>20</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17535</th>\n",
       "      <td>50</td>\n",
       "      <td>SUCT</td>\n",
       "      <td>Río Suchiate</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17536</th>\n",
       "      <td>3</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17537 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FREQUENCY BCODE          name waterway\n",
       "0              6  AKPA                  drain\n",
       "1             38  AKPA                  river\n",
       "2            100  AKPA                 stream\n",
       "3              3  AKPA          Afaẖ   stream\n",
       "4             49  AKPA     Akpakorum    river\n",
       "...          ...   ...           ...      ...\n",
       "17532          4  SUCT                  ditch\n",
       "17533          2  SUCT                  drain\n",
       "17534         20  SUCT                  river\n",
       "17535         50  SUCT  Río Suchiate    river\n",
       "17536          3  SUCT                 stream\n",
       "\n",
       "[17537 rows x 4 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_south = pd.concat([merged_df, overlap_df], ignore_index=True)\n",
    "new_south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1(b) clean them up a little - drop nans, drop non roman alphabet results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>BCODE</th>\n",
       "      <th>name</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Afaẖ</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpakorum</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpasang</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Ikpan</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Nkanya</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17522</th>\n",
       "      <td>39</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Hondo</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17523</th>\n",
       "      <td>3</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Ixnohá</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17524</th>\n",
       "      <td>4</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Seco</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17526</th>\n",
       "      <td>4</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Gold Button</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17535</th>\n",
       "      <td>50</td>\n",
       "      <td>SUCT</td>\n",
       "      <td>Suchiate</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16669 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FREQUENCY BCODE         name waterway\n",
       "3              3  AKPA         Afaẖ   stream\n",
       "4             49  AKPA    Akpakorum    river\n",
       "5             10  AKPA     Akpasang    river\n",
       "6             19  AKPA        Ikpan    river\n",
       "7             10  AKPA       Nkanya    river\n",
       "...          ...   ...          ...      ...\n",
       "17522         39  HOND        Hondo    river\n",
       "17523          3  HOND       Ixnohá    river\n",
       "17524          4  HOND         Seco    river\n",
       "17526          4  HOND  Gold Button   stream\n",
       "17535         50  SUCT     Suchiate    river\n",
       "\n",
       "[16669 rows x 4 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_dropna = new_south.dropna(subset=['name']) # this actually doesn't work ...\n",
    "#df_dropna = new_south[new_south.name != \" \"] # for some reason most of the nans are empty strings with a space\n",
    "df_dropna = new_south[new_south.name != \"\"] # so these are empty strings with no space\n",
    "df_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>BCODE</th>\n",
       "      <th>name</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>AKPA</td>\n",
       "      <td></td>\n",
       "      <td>drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>AKPA</td>\n",
       "      <td></td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>AKPA</td>\n",
       "      <td></td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Afaẖ</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpakorum</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17532</th>\n",
       "      <td>4</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>ditch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17533</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17534</th>\n",
       "      <td>20</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17535</th>\n",
       "      <td>50</td>\n",
       "      <td>SUCT</td>\n",
       "      <td>Suchiate</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17536</th>\n",
       "      <td>3</td>\n",
       "      <td>SUCT</td>\n",
       "      <td></td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17537 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FREQUENCY BCODE       name waterway\n",
       "0              6  AKPA               drain\n",
       "1             38  AKPA               river\n",
       "2            100  AKPA              stream\n",
       "3              3  AKPA       Afaẖ   stream\n",
       "4             49  AKPA  Akpakorum    river\n",
       "...          ...   ...        ...      ...\n",
       "17532          4  SUCT               ditch\n",
       "17533          2  SUCT               drain\n",
       "17534         20  SUCT               river\n",
       "17535         50  SUCT   Suchiate    river\n",
       "17536          3  SUCT              stream\n",
       "\n",
       "[17537 rows x 4 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably keep only terms in the roman alphabet since our terms list is too long\n",
    "import re\n",
    "\n",
    "'''\n",
    "def keep_roman_chars(text): # this was removing single letters from words, changing the name entirely\n",
    "    if isinstance(text, str):\n",
    "        # this keeps only Latin/Roman characters, numbers, and common punctuation\n",
    "        return re.sub(r'[^\\x00-\\x7F]+', '', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "#roman_africa_names = keep_roman_chars(Africa_names)\n",
    "#roman_africa_names # don't quite know what to do with an array\n",
    "'''\n",
    "\n",
    "# this one gets a little more context before it drops the characters\n",
    "\n",
    "def char_filter(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Process each word\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # Check if the word contains any ASCII characters\n",
    "        if re.search(r'[\\x00-\\x7F]', word):\n",
    "            # Word has some ASCII chars, keep it intact\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            # Word has only non-ASCII chars, skip it\n",
    "            continue\n",
    "    \n",
    "    # Join the filtered words back together\n",
    "    return ' '.join(processed_words).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/5nr18wp5321282mzd8gc8cbm0000gn/T/ipykernel_3341/1477255765.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dropna[\"name\"] = df_dropna[\"name\"].apply(char_filter)\n"
     ]
    }
   ],
   "source": [
    "df_dropna[\"name\"] = df_dropna[\"name\"].apply(char_filter)\n",
    "\n",
    "# I'm just getting rid of all the extra columns\n",
    "\n",
    "#df_dropna[df_dropna.name != \" \"] # maybe don't need to do this\n",
    "\n",
    "new_south = pd.DataFrame(df_dropna)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Afaẖ', 'Akpakorum', 'Akpasang', ..., 'Fogotico', 'La Florida',\n",
       "       'Brenquio'], shape=(13642,), dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_south['name'].unique() # why are there still nans dude?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: remove water-related terms\n",
    "make a list of words that we'll remove, because they'll already be captured with our water related search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2a) get words from the extracted OSM terms above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2b) get words from our water-related search terms (Alabama meeting Feb 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "febsearchterms = pd.read_excel(\"SearchTerms_Feb2025.xlsx\")\n",
    "\n",
    "box1_raw = febsearchterms.iloc[0, 1]\n",
    "\n",
    "def parse_terms(raw_terms):\n",
    "    # Split the string by OR and remove leading/trailing whitespace\n",
    "    terms = [term.strip() for term in box1_raw.split('OR')]\n",
    "    \n",
    "    # Remove asterisks and any extra whitespace\n",
    "    clean_terms = [re.sub(r'\\*', '', term).strip() for term in terms]\n",
    "    \n",
    "    # Remove any empty strings and get unique terms\n",
    "    unique_clean_terms = set(clean_terms)\n",
    "    \n",
    "    return unique_clean_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spring', 'creek', 'flood', 'riparian', 'tributar', 'pond', 'river', 'lake', 'drought', 'stream', 'hydroelect', 'dam', 'aquifer', 'reservoir', 'streams', 'water', 'wadi', 'oass', 'irrigat', 'canal', 'groundwater'}\n"
     ]
    }
   ],
   "source": [
    "parsed_box1terms = parse_terms(box1_raw)\n",
    "print(parsed_box1terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2c) add in some translations for major languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spring', 'réservoir', 'inundação', 'húbó', 'sécheresse', 'yùnhé', 'shuǐ', 'sequía', 'riparian', 'ribereño', 'paanee', 'sinchaee', 'zasukha', 'shuǐkù', 'khazan mayiy', 'água', 'zhīliú', 'baadh', 'pritok', 'drought', 'stream', 'barragem', 'dhaara', 'inondation', 'irrigação', 'kanal', 'aquifer', 'difafu', 'baandh', 'jifaf', 'irrigación', 'rwafdi', 'nascente', 'agua', 'vodonosnyy gorizont', 'eaux souterraines', 'vodokhranilishche', 'ruisseaux', 'streams', 'nakhalistaan', 'voda', 'wadi', 'kahrumayiyatun', 'rio', 'jheel', 'gānhàn', 'navodneniye', 'hidrelétrica', 'oued', 'canal', 'jharana', 'khaadee', 'uádi', 'uadi', 'creek', 'reka', 'agua subterránea', 'água subterrânea', 'flood', 'tyar', 'xīliú', 'héliú', 'tributar', 'hóngshuǐ', 'qnatu', \"ma'\", 'khazan', 'manantial', 'vaadee', 'lake', 'orosheniye', 'sad', 'riachos', 'shuǐlì fādiàn', 'eau', \"hé'àn\", 'fiadani', 'miah jufiatun', 'rivière', 'dìxiàshuǐ', 'jdawul', 'bhayratu', 'irrigat', 'taalaab', 'prud', 'barkata', 'wahatu', 'sahaayak nadee', 'río', 'vadi', 'ri', 'khur', 'acuífero', 'river', 'ozero', 'gruntovyye vody', 'aquífero', 'estanque', 'arroyos', 'afluente', 'nbae', 'ribeirão', 'barrage', 'reservoir', 'jalabhrt', 'gidroelektrostantsiya', 'water', 'nahar', 'aquifère', 'istochnik', 'plotina', 'lagoa', 'seca', 'oazis', 'groundwater', 'ruchey', \"ruch'i\", 'tatavartee', 'riacho', 'sookha', 'dhaaraen', 'irrigation', 'pribrezhnyy', 'nhar', 'hidroeléctrica', 'lago', 'lac', 'ruisseau', 'jalaashay', 'pond', 'hánshuǐ céng', 'inundación', 'chítáng', 'étang', 'presa', 'xiǎo xī', 'shuǐbà', 'hydroelect', 'reservatório', 'nadee', 'dam', 'arroyo', 'bhoojal', 'hydroélectricité', 'dren', 'quebrada', 'guàngài', 'oass', 'jalavidyut', 'gàn gǔ', 'affluent', 'oásis', 'Quánshuǐ', 'lǜzhōu', 'embalse', 'rafid'}\n"
     ]
    }
   ],
   "source": [
    "# went to google translate, asked for these terms above in the following common languages. removed duplicates.\n",
    "# noting that I filled in *s (oas*s to oasis, for example) - this is where we could use help from a native speaker\n",
    "box1_arabic = {'difafu', 'tyar', 'wahatu', 'khazan mayiy', \"ma'\", 'nhar', 'khur', 'jdawul', 'nbae', 'bhayratu', 'fiadani', 'miah jufiatun', 'kahrumayiyatun', 'khazan', 'ri', 'wadi', 'sad', 'barkata', 'jifaf', 'rafid', 'rwafdi', 'qnatu'}\n",
    "box1_spanish = {'sequía', 'lago', 'inundación', 'arroyo', 'acuífero', 'ribereño', 'agua', 'manantial', 'hidroeléctrica', 'arroyo', 'arroyos', 'presa', 'irrigación', 'embalse', 'afluente', 'agua subterránea', 'río', 'estanque', 'uadi', 'quebrada', 'dren'}\n",
    "box1_portuguese = {'seca', 'inundação', 'riacho', 'aquífero', 'ribeirão', 'água', 'nascente', 'hidrelétrica', 'riacho', 'riachos', 'barragem', 'irrigação', 'reservatório', 'água subterrânea', 'rio', 'lagoa', 'uádi', 'oásis'}\n",
    "box1_french = {'sécheresse', 'lac', 'inondation', 'ruisseau', 'aquifère', 'rivière', 'eau', 'hydroélectricité', 'ruisseau', 'ruisseaux', 'barrage', 'irrigation', 'réservoir', 'affluent', 'eaux souterraines', 'rivière', 'étang', 'oued'}\n",
    "box1_mandarin = {'Quánshuǐ','xiǎo xī','hóngshuǐ', \"hé'àn\", 'zhīliú', 'chítáng', 'héliú', 'húbó', 'gānhàn', 'xīliú', 'shuǐlì fādiàn','shuǐbà','hánshuǐ céng','shuǐkù','xīliú','shuǐ', 'gàn gǔ','lǜzhōu','guàngài','yùnhé','dìxiàshuǐ'}\n",
    "box1_hindi = {\"jharana\", \"khaadee\", \"baadh\", \"tatavartee\", \"sahaayak nadee\", \"taalaab\", \"nadee\", \"jheel\", \"sookha\", \"dhaara\", \"jalavidyut\", \"baandh\", \"jalabhrt\", \"jalaashay\", \"dhaaraen\", \"paanee\", \"vaadee\", \"nakhalistaan\", \"sinchaee\", \"nahar\", \"bhoojal\"}\n",
    "box1_russian = {'istochnik', 'ruchey', 'navodneniye', 'pribrezhnyy', 'pritok', 'prud', 'reka', 'ozero', 'zasukha', 'ruchey', 'gidroelektrostantsiya', 'plotina', 'vodonosnyy gorizont', 'vodokhranilishche', \"ruch'i\", 'voda', 'vadi', 'oazis', 'orosheniye', 'kanal', 'gruntovyye vody'}\n",
    "\n",
    "# add arabic to parsed box 1\n",
    "box1_translations = parsed_box1terms | box1_arabic | box1_spanish | box1_portuguese | box1_french | box1_mandarin | box1_hindi | box1_russian\n",
    "# this | operator works like a + but with a set not a list\n",
    "print(box1_translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## then step 3, going through the dataframe name column and removing any of the box 1 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is older, and works on a list. the next cell has one that can work on single strings and be used through rows in dataframe\n",
    "\n",
    "def clean_string(s, terms_to_remove):\n",
    "    \"\"\"\n",
    "    Remove specified terms from within a string, case-insensitively,\n",
    "    ensuring proper spacing after removal.\n",
    "    \n",
    "    Args:\n",
    "        s (str): String to process\n",
    "        terms_to_remove (list): Terms to remove\n",
    "    \n",
    "    Returns:\n",
    "        str: Processed string with terms removed\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    \n",
    "    # Split into words\n",
    "    words = s.split()\n",
    "    \n",
    "    # Filter out words that match any term in terms_to_remove (case-insensitive)\n",
    "    cleaned_words = [\n",
    "        word for word in words \n",
    "        if word.lower() not in [term.lower() for term in terms_to_remove]\n",
    "    ]\n",
    "    \n",
    "    # Join back with a single space\n",
    "    return ' '.join(cleaned_words).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply to your DataFrame column using lambda with your list of terms to remove\n",
    "terms_to_remove = box1_translations\n",
    "\n",
    "# Apply to the DataFrame column\n",
    "new_south['name'] = new_south['name'].apply(lambda x: clean_string(x, terms_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AKPA', 'ALHN', 'ANNL', 'AWSH', 'BENT', 'BIAX', 'BNSA', 'BRKA',\n",
       "       'BUZI', 'CLNG', 'CNGO', 'CROS', 'CSTO', 'CVLY', 'DAUR', 'DRAX',\n",
       "       'ETOS', 'GAMB', 'GASH', 'GEBA', 'GLNA', 'GSCR', 'GUIR', 'ICMT',\n",
       "       'JUBA', 'KMOE', 'KUNE', 'LGPS', 'LKCH', 'LKCW', 'LKCY', 'LKDR',\n",
       "       'LKNT', 'LKRW', 'LKTK', 'LMPO', 'LOFF', 'LSCR', 'MANA', 'MBEX',\n",
       "       'MDJD', 'MOAX', 'MONO', 'MPUT', 'NGER', 'NILE', 'NYGA', 'ODBN',\n",
       "       'OGOO', 'OKVG', 'ORAN', 'OUEM', 'PANG', 'PUNG', 'RVMA', 'SABI',\n",
       "       'SANA', 'SASS', 'SENG', 'SJAF', 'SPAU', 'TAFN', 'TANO', 'UBLZ',\n",
       "       'UMBA', 'UTBN', 'VOLT', 'ZAMB', 'ATBN', 'BLZE', 'CDLR', 'CGNL',\n",
       "       'CHLT', 'COCO', 'CTAT', 'GJLV', 'GOSR', 'HOND', 'JURD', 'LKAZ',\n",
       "       'LKEQ', 'LMPA', 'MASS', 'MOHO', 'MOTQ', 'NEGR', 'PAZX', 'PDNL',\n",
       "       'SIOL', 'SJUA', 'SRTU', 'SUCT', 'TEMA', 'AMCR', 'AMZN', 'AYSN',\n",
       "       'BAKR', 'BRMA', 'CHIC', 'CHIR', 'CHUY', 'CNCS', 'COMA', 'CRTY',\n",
       "       'CTTB', 'CULL', 'ESQB', 'GALG', 'LKFN', 'LKTC', 'LMRM', 'LOAX',\n",
       "       'LPTA', 'MIRA', 'MRNI', 'MTJE', 'ORIN', 'OYPK', 'PLNA', 'PSCU',\n",
       "       'PTIA', 'PUEL', 'RGSA', 'SENO', 'SMAR', 'TUMB', 'VDVA', 'YELC',\n",
       "       'ZAPL', 'ZARM'], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_south['BCODE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>BCODE</th>\n",
       "      <th>name</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Afaẖ</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpakorum</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpasang</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Ikpan</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Nkanya</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17522</th>\n",
       "      <td>39</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Hondo</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17523</th>\n",
       "      <td>3</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Ixnohá</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17524</th>\n",
       "      <td>4</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Seco</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17526</th>\n",
       "      <td>4</td>\n",
       "      <td>HOND</td>\n",
       "      <td>Gold Button</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17535</th>\n",
       "      <td>50</td>\n",
       "      <td>SUCT</td>\n",
       "      <td>Suchiate</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16669 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FREQUENCY BCODE         name waterway\n",
       "3              3  AKPA         Afaẖ   stream\n",
       "4             49  AKPA    Akpakorum    river\n",
       "5             10  AKPA     Akpasang    river\n",
       "6             19  AKPA        Ikpan    river\n",
       "7             10  AKPA       Nkanya    river\n",
       "...          ...   ...          ...      ...\n",
       "17522         39  HOND        Hondo    river\n",
       "17523          3  HOND       Ixnohá    river\n",
       "17524          4  HOND         Seco    river\n",
       "17526          4  HOND  Gold Button   stream\n",
       "17535         50  SUCT     Suchiate    river\n",
       "\n",
       "[16669 rows x 4 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then I need to add this to the \n",
    "# old_south = pd.read_excel('AF_CA_SA_basin_terms.xlsx')\n",
    "# updated_south = pd.concat([old_south, dropna_clean_df])\n",
    "\n",
    "# # probably sort by bcode so they're all together, though maybe it doesn't matter\n",
    "new_south = new_south.sort_values(by='BCODE')\n",
    "new_south.to_excel('udpated_south_pre-edit.xlsx') # excel deals better with non-ascii than csv?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stream', 'river', '    river', '    stream', 'drain', 'canal',\n",
       "       'dam', 'ditch', 'tidal_channel', '    ditch', '    canal',\n",
       "       '    drain', 'rapids', 'fairway', '    weir', 'waterfall', 'weir',\n",
       "       'dock', '    dam', 'wadi', '    waterfall', 'planned_canal'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_south['waterway'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_bcodes = new_south[\"BCODE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(49)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_south.iloc[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_south.iloc[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>BCODE</th>\n",
       "      <th>name</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17155</th>\n",
       "      <td>7</td>\n",
       "      <td>SENO</td>\n",
       "      <td>de las Chinas</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17159</th>\n",
       "      <td>1</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Diente</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17152</th>\n",
       "      <td>1</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Ascencio</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17153</th>\n",
       "      <td>11</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Baguales</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17151</th>\n",
       "      <td>4</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Zanja Honda</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17160</th>\n",
       "      <td>15</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Don Guillermo</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17157</th>\n",
       "      <td>1</td>\n",
       "      <td>SENO</td>\n",
       "      <td>del Arriero</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17150</th>\n",
       "      <td>2</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Ensenada de Falcón</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17158</th>\n",
       "      <td>1</td>\n",
       "      <td>SENO</td>\n",
       "      <td>del Francés</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17149</th>\n",
       "      <td>3</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Cazador</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17165</th>\n",
       "      <td>21</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Serrano</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17154</th>\n",
       "      <td>2</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Campamento</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17164</th>\n",
       "      <td>11</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Pingo</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17161</th>\n",
       "      <td>5</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Grey</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17166</th>\n",
       "      <td>12</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Vizcachas</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17163</th>\n",
       "      <td>17</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Paine</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17156</th>\n",
       "      <td>1</td>\n",
       "      <td>SENO</td>\n",
       "      <td>de los Perros</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17162</th>\n",
       "      <td>21</td>\n",
       "      <td>SENO</td>\n",
       "      <td>Las Vizcachas</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FREQUENCY BCODE                name waterway\n",
       "17155          7  SENO       de las Chinas    river\n",
       "17159          1  SENO              Diente    river\n",
       "17152          1  SENO            Ascencio    river\n",
       "17153         11  SENO            Baguales    river\n",
       "17151          4  SENO         Zanja Honda    river\n",
       "17160         15  SENO       Don Guillermo    river\n",
       "17157          1  SENO         del Arriero    river\n",
       "17150          2  SENO  Ensenada de Falcón    river\n",
       "17158          1  SENO         del Francés    river\n",
       "17149          3  SENO             Cazador   stream\n",
       "17165         21  SENO             Serrano    river\n",
       "17154          2  SENO          Campamento   stream\n",
       "17164         11  SENO               Pingo    river\n",
       "17161          5  SENO                Grey    river\n",
       "17166         12  SENO           Vizcachas    river\n",
       "17163         17  SENO               Paine    river\n",
       "17156          1  SENO       de los Perros    river\n",
       "17162         21  SENO       Las Vizcachas    river"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_south[new_south['BCODE']=='SENO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Akpa', 'Amacuro', 'Amazon', 'Annole', 'Artibonite', 'Awash',\n",
       "       'Aysen', 'Baker', 'Benito/Ntem', 'Bia', 'Belize',\n",
       "       'Oued Bou Namoussa', 'Baraka', 'Barima', 'Buzi', 'Candelaria',\n",
       "       'Changuinola', 'Carmen Silva/Chico', 'Chira', 'Choluteca', 'Chuy',\n",
       "       'Chiloango', 'Cancoso/Lauca', 'Congo/Zaire', 'Coco/Segovia',\n",
       "       'Comau', 'Cross', 'Corantijn/Courantyne', 'Cestos',\n",
       "       'Coatan Achute', 'Catatumbo', 'Cullen', 'Cavally', 'Daoura', 'Dra',\n",
       "       'Essequibo', 'Cuvelai/Etosha', 'Gallegos/Chico', 'Gambia', 'Gash',\n",
       "       'Geba-Corubal', 'Grijalva', 'Galana', 'Goascoran',\n",
       "       'Great Scarcies', 'Guir', 'Hondo', 'Incomati', 'Juba-Shibeli',\n",
       "       'Jurado', 'Komoe', 'Kunene', 'Lotagipi Swamp', 'Lake Azuei',\n",
       "       'Lake Chad', 'Lake Chilwa', 'Lake Cayo', 'Lak Dera',\n",
       "       'Lake Enriquillo', 'Lake Fagnano', 'Lake Natron', 'Lake Rukwa',\n",
       "       'Lake Titicaca-Poopo System', 'Lake Turkana', 'Lempa', 'Limpopo',\n",
       "       'Lagoon Dos Patos-Lagoon Mirim', 'Loffa', 'La Plata',\n",
       "       'Little Scarcies', 'Mana-Morro', 'Massacre', 'Mbe', 'Medjerda',\n",
       "       'Mira', 'Moa', 'Moho', 'Mono', 'Motaqua', 'Maputo', 'Maroni',\n",
       "       'Mataje', 'Negro', 'Niger', 'Nile', 'Nyanga', 'Oued Bon Naima',\n",
       "       'Ogooue', 'Okavango', 'Orange', 'Orinoco', 'Oueme',\n",
       "       'Oiapoque/Oyupock', 'Pangani', 'Paz', 'Pedernales', 'Palena',\n",
       "       'Pascua', 'Patia', 'Puelo', 'Pungwe', 'Rio Grande (South America)',\n",
       "       'Ruvuma', 'Sabi', 'Sanaga', 'Sassandra', 'Senegal',\n",
       "       'Seno Union/Serrano', 'Sixaola', 'St. John (Africa)', 'San Juan',\n",
       "       'San Martin', 'St. Paul', 'Sarstun', 'Suchiate', 'Tafna', 'Tano',\n",
       "       'Temash', 'Tumbes', 'Umbeluzi', 'Umba', 'Utamboni', 'Valdivia',\n",
       "       'Volta', 'Yelcho', 'Zambezi', 'Zapaleri', 'Zarumilla', 'Allahein',\n",
       "       'Loa'], dtype=object)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms_official = pd.read_excel('/Users/selenawallace/Documents/geography/geography/search_terms.xlsx')\n",
    "searchterms_south = search_terms_official[search_terms_official['BCODE'].isin(south_bcodes)]\n",
    "searchterms_south['Basin_Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- step 3.5, do some manual cleaning before proceeding\n",
    "\n",
    "## step 4, deal with duplicates, sort by frequency, bin by waterway type\n",
    "-\n",
    "steps to insert in the if/then statement checking search string length\n",
    "* check for duplicates (last, after new string, sorting and binning)\n",
    "* create new string BCODE-name-waterway\n",
    "* sort by frequency (within bins)\n",
    "* bin waterway types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>BCODE</th>\n",
       "      <th>name</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Afaẖ</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpakorum</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Akpasang</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Ikpan</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>AKPA</td>\n",
       "      <td>Nkanya</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>2</td>\n",
       "      <td>ZARM</td>\n",
       "      <td>Chiquita</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16669</th>\n",
       "      <td>2</td>\n",
       "      <td>ZARM</td>\n",
       "      <td>Las Lajas</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16670</th>\n",
       "      <td>6</td>\n",
       "      <td>ZARM</td>\n",
       "      <td>Palmales</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16671</th>\n",
       "      <td>2</td>\n",
       "      <td>ZARM</td>\n",
       "      <td>Faical</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16672</th>\n",
       "      <td>1</td>\n",
       "      <td>ZARM</td>\n",
       "      <td>Canoas</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16673 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FREQUENCY BCODE       name waterway\n",
       "0              3  AKPA       Afaẖ   stream\n",
       "1             49  AKPA  Akpakorum    river\n",
       "2             10  AKPA   Akpasang    river\n",
       "3             19  AKPA      Ikpan    river\n",
       "4             10  AKPA     Nkanya    river\n",
       "...          ...   ...        ...      ...\n",
       "16668          2  ZARM   Chiquita    river\n",
       "16669          2  ZARM  Las Lajas    river\n",
       "16670          6  ZARM   Palmales   stream\n",
       "16671          2  ZARM     Faical   stream\n",
       "16672          1  ZARM     Canoas   stream\n",
       "\n",
       "[16673 rows x 4 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cleaned from 3.5\n",
    "south_cleaned = pd.read_excel(\"updated_south.xlsx\")\n",
    "# reset index\n",
    "south_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bin/categorize waterway types first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "priority_map = {\n",
    "        'river': 1,\n",
    "        'stream': 2,\n",
    "        'canal': 3,\n",
    "        'dam': 4,\n",
    "        'fairway' : 5\n",
    "    }\n",
    "\n",
    "# Create new column using map with a default value\n",
    "south_cleaned['waterway_priority'] = south_cleaned['waterway'].map(priority_map).fillna(6).astype(int)\n",
    "# and all not in map (\"low priority\" waterways) will be 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that concatenates the strings in three columns (add - delimiter) BCODE-name-waterway\n",
    "south_cleaned['combined'] = south_cleaned['BCODE'] + '-' + south_cleaned['name'] + '-' + south_cleaned['waterway_priority'].astype(str)\n",
    "#south_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate(df):\n",
    "    # Step 1: Deduplicate by 'combined' and sum 'FREQUENCY'\n",
    "    frequency_sums = df.groupby('combined')['FREQUENCY'].sum().reset_index()\n",
    "    freq_dict = dict(zip(frequency_sums['combined'], frequency_sums['FREQUENCY']))\n",
    "    \n",
    "    # Select first row for each unique 'combined' value\n",
    "    deduplicated_df = df.groupby('combined', as_index=False).first()\n",
    "    \n",
    "    # Update 'FREQUENCY' column with summed frequencies\n",
    "    deduplicated_df['FREQUENCY'] = deduplicated_df['combined'].map(freq_dict)\n",
    "    \n",
    "    # Step 2: Calculate 'basin_freq_total' based on 'BCODE'\n",
    "    basin_totals = deduplicated_df.groupby('BCODE')['FREQUENCY'].sum().reset_index()\n",
    "    basin_dict = dict(zip(basin_totals['BCODE'], basin_totals['FREQUENCY']))\n",
    "    \n",
    "    # Add the 'basin_freq_total' column\n",
    "    deduplicated_df['basin_freq_total'] = deduplicated_df['BCODE'].map(basin_dict)\n",
    "    \n",
    "    return deduplicated_df\n",
    "\n",
    "deduplicated_df = deduplicate(south_cleaned)\n",
    "#deduplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then normalize\n",
    "deduplicated_df['normalized_frequency'] = deduplicated_df['FREQUENCY']/deduplicated_df['basin_freq_total']\n",
    "#deduplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = deduplicated_df.sort_values(by=['BCODE', 'waterway_priority', 'normalized_frequency'], ascending=[True, True, False])\n",
    "#sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes for north\n",
    "* check what languages nexis uni will accept in search terms\n",
    "* frequency priority and waterway weight as before\n",
    "* second weight for nonroman vs roman characters (prioritize roman, cut nonroman)\n",
    "\n",
    "* compare to TFDD previous search terms (which should be sorted priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe just put it in a sheet normalized_south so it isn't gone forever lol\n",
    "sorted_df.to_excel(\"normalized_south.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5, make the names into strings and check character limit\n",
    "- each basin will have a unique cutoff point, cut off based on step 4 ordering/binning\n",
    "- put it in a spreadsheet the script will be able to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_search_string(terms_list):\n",
    "    # Since terms are already cleaned, just filter out empty/too short terms\n",
    "    filtered_terms = []\n",
    "    \n",
    "    for term in terms_list:\n",
    "        if isinstance(term, str) and term.strip() and len(term.strip()) > 1:\n",
    "            filtered_terms.append(term.strip())\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_terms = []\n",
    "    for term in filtered_terms:\n",
    "        if term not in unique_terms:\n",
    "            unique_terms.append(term)\n",
    "    \n",
    "    # Format terms - add quotes only for multi-word terms\n",
    "    formatted_terms = []\n",
    "    for term in unique_terms:\n",
    "        if ' ' in term:\n",
    "            formatted_terms.append(f'\"{term}\"')  # Add quotes for multi-word terms\n",
    "        else:\n",
    "            formatted_terms.append(term)  # No quotes for single words\n",
    "    \n",
    "    # Join with OR\n",
    "    search_string = \" OR \".join(formatted_terms)\n",
    "    \n",
    "    return search_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # testing for length\n",
    "# febsearchterms = pd.read_excel(\"SearchTerms_Feb2025.xlsx\") # this is in gitignore now\n",
    "\n",
    "# box1 = febsearchterms.iloc[0, 1]\n",
    "# box2 = febsearchterms.iloc[1, 1]\n",
    "# #box3 = basin_specific_terms\n",
    "# box4 = febsearchterms.iloc[4, 1] \n",
    "\n",
    "# string_wo_box3 = 'hlead(' + box1 + ') and hlead(' + box2 + ') and hlead(' + ') and not hlead(' + box4 + ')' # not including + box3\n",
    "\n",
    "# box3_limit = 5000 - (len(string_wo_box3))\n",
    "\n",
    "# box3_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I ever get rid of the above or move feb terms, just have it stored here, uncomment and run this\n",
    "box3_limit = 3460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_data = []\n",
    "#basins_to_trim = []\n",
    "\n",
    "# a list of all the unique bcodes\n",
    "basins_with_terms = sorted_df[\"BCODE\"].unique()\n",
    "\n",
    "for basin in basins_with_terms: # for each basin in that list\n",
    "    df = sorted_df[sorted_df[\"BCODE\"]== basin] #slice the dataframe so it's just the names in that basin\n",
    "    list = df['name'].unique().tolist() # and make a list of \n",
    "    search_string = create_search_string(list) # this will make a search string out of it using the method above\n",
    "\n",
    "    #then we check it for character limit\n",
    "    if len(search_string)>box3_limit: # if it's too long\n",
    "\n",
    "        truncated = search_string[:box3_limit] # trims it to exactly the limit\n",
    "        last_or_pos = truncated.rfind(\" OR \") # finds the nearest 'or'\n",
    "        search_string = search_string[:last_or_pos] # truncates from there so the final term is a complete one\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    search_terms_data.append({'basin': basin, 'basin_specific_terms': search_string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_south = pd.DataFrame(search_terms_data)\n",
    "search_terms_south.to_excel(\"south_searchterms_May12.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from here below is old, but may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort terms by frequency & frequency null (or make null high '999') high to low\n",
    "# bin frequency potentially , consider hierarchy of different waterway types\n",
    "# and then figure out how much to trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in these basins (each?), get count of unique frequency, count of unique waterway\n",
    "# could get rid of ditch, stream with 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ADIG\n",
       "1      AKPA\n",
       "2      ALKL\n",
       "3      ALSK\n",
       "4      AMCR\n",
       "       ... \n",
       "309    ZARM\n",
       "310    ALHN\n",
       "311    EMSX\n",
       "312    LOAX\n",
       "313    GRND\n",
       "Name: BCODE, Length: 314, dtype: object"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another manual clean step, make sure TFDD name is in the list of names from OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## and put the new search terms into a new sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original search terms sheet (in repository)\n",
    "searchterms_path = '/Users/selenawallace/Documents/geography/geography/basins_searchterms_tracking.xlsx'# hard-coding the path is fine because I'm the only one running this, I think\n",
    "search_terms_official = pd.read_excel(searchterms_path)\n",
    "\n",
    "\n",
    "search_terms_south # is the new \n",
    "\n",
    "\n",
    "# Iterate through your new dataframe and update the original\n",
    "for index, row in search_terms_south.iterrows():\n",
    "    bcode = row['basin']\n",
    "    new_terms = row['basin_specific_terms']\n",
    "    \n",
    "    # Find matching rows in the original dataframe by BCODE\n",
    "    mask = search_terms_official['BCODE'] == bcode\n",
    "    \n",
    "    # Update if there's a match\n",
    "    if mask.any():\n",
    "        # Update the Basin_Specific_Terms\n",
    "        search_terms_official.loc[mask, 'Basin_Specific_Terms'] = new_terms\n",
    "        \n",
    "        # Mark as updated\n",
    "        search_terms_official.loc[mask, 'Terms_updated_2025'] = 'Y'  \n",
    "\n",
    "# Save the updated spreadsheet\n",
    "search_terms_official.to_excel(searchterms_path, index=False)  # or .csv if preferred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## older"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not necessary right now but was helpful \n",
    "\n",
    "# check for spaces in 'name' first\n",
    "\n",
    "def check_spaces(df, column_name):\n",
    "    \"\"\"\n",
    "    Checks for leading and trailing spaces in a specified column of a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column to check.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing rows with leading or trailing spaces in the specified column.\n",
    "                          Returns an empty DataFrame if no spaces are found.\n",
    "    \"\"\"\n",
    "    space_mask = df[column_name].str.contains(r'^\\s+|\\s+$', na=False)\n",
    "    return df[space_mask]\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "df = pd.DataFrame(south_cleaned)\n",
    "\n",
    "column_to_check = 'name'\n",
    "rows_with_spaces = check_spaces(df, column_to_check)\n",
    "\n",
    "if not rows_with_spaces.empty:\n",
    "    print(f\"Rows with leading/trailing spaces in '{column_to_check}':\")\n",
    "    print(rows_with_spaces)\n",
    "else:\n",
    "    print(f\"No leading/trailing spaces found in '{column_to_check}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
